{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following codes were used to train the two custom object detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2325,
     "status": "ok",
     "timestamp": 1588458345958,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "MWP3GdzC3_sk",
    "outputId": "800d3d22-bf8e-41f5-e5aa-3e4b8f825f9c"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dropout, LeakyReLU, Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Concatenate, concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import PIL\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.keras import Input, Model,utils, Sequential\n",
    "from tensorflow.keras.layers import Concatenate, AvgPool2D, GlobalAvgPool2D,  MaxPool2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ih9v2u0L4qCp"
   },
   "source": [
    "# Model preparation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPT-xiIx_qvh"
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Nn1Uhc9_t2-"
   },
   "outputs": [],
   "source": [
    "def dense_block(x, block_length ,depth):\n",
    "    '''\n",
    "    Arguments: \n",
    "    input: input tensor\n",
    "    block_length: length of block \n",
    "    depth: number of channels for each conv layer in the block\n",
    "\n",
    "    '''\n",
    "\n",
    "    for i in range(block_length):\n",
    "        x = conv_block(x, depth)\n",
    "    return x\n",
    "\n",
    "def conv_block(x, depth):\n",
    "\n",
    "    '''\n",
    "    Arguments: \n",
    "    input: input tensor \n",
    "    depth: number of channels for each conv layer in the block\n",
    "\n",
    "    '''\n",
    "\n",
    "    x1 = BatchNormalization(axis=-1)(x)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Conv2D(depth, 3, padding='same')(x1)\n",
    "    x = Concatenate(axis=-1)([x,x1])\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def transition_block(x):\n",
    "\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(int(backend.int_shape(x)[3]), (1,1))(x)\n",
    "    x = AvgPool2D((2,2), strides=2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_densenet():\n",
    "\n",
    "\n",
    "    img_input = Input((416,416,3),dtype='float32')\n",
    "    x = Conv2D(64, (7,7), (2,2), padding='same')(img_input)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D((3,3), (2,2), padding='same')(x)\n",
    "\n",
    "    x = dense_block(x, 4, 32) # DenseBlock1\n",
    "    x = transition_block(x) # TransitionBlock1\n",
    "\n",
    "    x = dense_block(x,4,32) # DenseBlock2\n",
    "    x = transition_block(x) #TransitionBlock2\n",
    "\n",
    "    x = dense_block(x,4,32) # DenseBlock3\n",
    "    x = transition_block(x) #TransitionBlock3\n",
    "\n",
    "    x = dense_block(x,4,32) # DenseBlock4\n",
    "\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = GlobalAvgPool2D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    model = Model(img_input, x, name='densenet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKorHHOe3_fj"
   },
   "outputs": [],
   "source": [
    "model = build_densenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1588458357289,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "BmAoxTXx3_Y_",
    "outputId": "12a14ff2-2e8b-416d-ad0d-bcd5da5c1dc6"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2QVyZPN3_Vr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=10e-8)\n",
    "model.compile(optimizer = optimizer,\n",
    "                loss = 'sparse_categorical_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "model.load_weights('weights.48-0.70.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbIRdgOFBMiZ"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2otEwGJ13_LH"
   },
   "outputs": [],
   "source": [
    "base_model = Model(inputs=model.layers[0].input, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdI3iZVM3_IC"
   },
   "outputs": [],
   "source": [
    "labels = ('pedestrian','vehicle')\n",
    "im_h = 416\n",
    "im_w = 416\n",
    "num_anchors = 5\n",
    "num_class = len(labels)\n",
    "anchors = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "grid_w = 13\n",
    "grid_h = 13\n",
    "\n",
    "lr = LeakyReLU(alpha=0.1)(base_model.output)\n",
    "dl = Conv2D(num_anchors*(4+1+num_class),(1,1),strides=(1,1), padding = 'same', name='detection_layer')(lr)\n",
    "output = Reshape((grid_h, grid_w, num_anchors, 4+1+num_class))(dl)\n",
    "new_model = Model(inputs = base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1588458380965,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "6a8EsiWX3_DT",
    "outputId": "23dbe6b3-77a1-4e22-f9f0-1111c9bbc2c6"
   },
   "outputs": [],
   "source": [
    "base_model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1588458386474,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "cEQ2Vsxd3-9I",
    "outputId": "9e56f447-973a-4da4-9e96-787174d9724d"
   },
   "outputs": [],
   "source": [
    "new_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0IaO0ne03-4g"
   },
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQUbx1EbDZGM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHUyJK6cDZrg"
   },
   "source": [
    "# Training data prep..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdnbml4C3-1D"
   },
   "outputs": [],
   "source": [
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "boxes = np.load('may_boxes.npy')\n",
    "np.load = np_load_old\n",
    "boxes = list(boxes)\n",
    "\n",
    "# data for first 1000 images only\n",
    "boxes = boxes[:5000]\n",
    "\n",
    "# convert bbox array to have same shape for each image data\n",
    "# accomodate for the maximum annotations for the dataset \n",
    "# put zeros for extra annotations\n",
    "\n",
    "ann_shape = []\n",
    "for i in boxes:\n",
    "    ann_shape.append(i.shape[0]) # 0th position holds the number of annotations per image\n",
    "\n",
    "max_ann_shape = max(ann_shape)\n",
    "equal_shape_boxes = np.zeros((5000,max_ann_shape,num_anchors))\n",
    "\n",
    "for index, box in enumerate(boxes):\n",
    "    equal_shape_boxes[index,:box.shape[0],:num_anchors] = box # saving the annotations for each image and allowing remaining annotations to be zero \n",
    "\n",
    "\n",
    "\n",
    "# prepare and load the image data\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "fn = np.load('may_fns.npy')\n",
    "np.load = np_load_old\n",
    "\n",
    "\n",
    "fns = fn.copy()\n",
    "fns = ['images/'+ i for i in fns]\n",
    "fns = fns[:5000]\n",
    "file_names = np.array(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orY-9lUp3-u1"
   },
   "outputs": [],
   "source": [
    "# create ground truth labels for training and processing results \n",
    "\n",
    "h_norm = 1/720\n",
    "w_norm = 1/1280\n",
    "anchors = np.array(anchors)\n",
    "anchors = anchors.reshape(num_anchors, 2)\n",
    "\n",
    "#\n",
    "all_matching_boxes = []\n",
    "all_detector_masks = []\n",
    "all_bboxes = []\n",
    "\n",
    "for i in range(equal_shape_boxes.shape[0]):\n",
    "\n",
    "    detector_mask = np.zeros((grid_w,grid_h,num_anchors,1))\n",
    "    matching_boxes = np.zeros((grid_w,grid_h,num_anchors,5))\n",
    "    grid_boxes = np.zeros(equal_shape_boxes.shape[1:])\n",
    "  \n",
    "    for j,b in  enumerate(equal_shape_boxes[i,:,:]):\n",
    "    \n",
    "    #convert pixel values annotations to yolo output resolution \n",
    "        w = (b[2] - b[0]) * w_norm * grid_w\n",
    "        h = (b[3] - b[1]) * h_norm * grid_h   \n",
    "        x = ((b[0] + b[2]) / 2) * w_norm * grid_w\n",
    "        y = ((b[1] + b[3]) / 2) * h_norm * grid_h\n",
    "        grid_boxes[j,...] = np.array([x,y,w,h,b[4]])\n",
    "\n",
    "        if x > 0: # if box annotation exists\n",
    "\n",
    "          # picking the best anchor for given bbox\n",
    "            iou_best = 0\n",
    "            anchor_best = 0\n",
    "            for k in range(num_anchors):\n",
    "                intersection = np.minimum(w,anchors[k,0]) * np.minimum(h,anchors[k,1])\n",
    "                union = (anchors[k,0] * anchors[k,1]) + (w * h) - intersection\n",
    "                iou = intersection / union\n",
    "\n",
    "                if iou > iou_best:\n",
    "                    iou_best = iou\n",
    "                    anchor_best = k\n",
    "            if iou_best > 0:\n",
    "\n",
    "                o_x = np.floor(x).astype('int')\n",
    "                o_y = np.floor(y).astype('int')\n",
    "                detector_mask[o_y, o_x, anchor_best] = 1\n",
    "                matching_boxes[o_y, o_x, anchor_best] = grid_boxes[j]\n",
    "\n",
    "    all_matching_boxes.append(matching_boxes)\n",
    "    all_detector_masks.append(detector_mask)\n",
    "    all_bboxes.append(grid_boxes)\n",
    "\n",
    "detector_masks = tf.convert_to_tensor(np.array(all_detector_masks), dtype='float32')\n",
    "matching_bboxes = tf.convert_to_tensor(np.array(all_matching_boxes),dtype='float32')\n",
    "bboxes = tf.convert_to_tensor(np.array(all_bboxes), dtype='float32')\n",
    "\n",
    "matching_classes = K.cast(matching_bboxes[..., 4], 'int32') \n",
    "class_one_hot = K.one_hot(matching_classes, num_class + 1)[:,:,:,:,1:]\n",
    "class_one_hot = tf.cast(class_one_hot, dtype='float32')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqy4RF8u3-rZ"
   },
   "outputs": [],
   "source": [
    "def parse_image_1(obj, detector_masks, matching_bboxes, class_one_hot, bboxes):\n",
    "    img_obj = tf.io.read_file(obj)\n",
    "    imgs = tf.image.decode_png(img_obj, channels=3)\n",
    "    imgs = tf.image.convert_image_dtype(imgs, tf.float32)\n",
    "    imgs = tf.image.resize(imgs,[416,416])\n",
    "    return imgs, detector_masks, matching_bboxes, class_one_hot, bboxes\n",
    "\n",
    "dataset_1 = tf.data.Dataset.from_tensor_slices((file_names,detector_masks, matching_bboxes, class_one_hot, bboxes))\n",
    "dataset_1 = dataset_1.map(parse_image_1, num_parallel_calls=6)\n",
    "dataset_1 = dataset_1.shuffle(len(file_names), reshuffle_each_iteration=True)\n",
    "dataset_1 = dataset_1.repeat()\n",
    "dataset_1 = dataset_1.batch(25)\n",
    "dataset_1 = dataset_1.prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1588458405402,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "FkrCHfkK3-mn",
    "outputId": "eb8285de-e03c-43df-a9ce-bc634531f84b"
   },
   "outputs": [],
   "source": [
    "detector_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1588458406256,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "IfmTFPKt3-iM",
    "outputId": "d5f854da-a73e-47d9-9962-1d64e25004a7"
   },
   "outputs": [],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1588458407377,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "mnY1_GevJ6hx",
    "outputId": "78a0e3d8-9b55-4499-8d21-45b8590a4263"
   },
   "outputs": [],
   "source": [
    "class_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1588435891595,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "Xz1863TCJ9kX",
    "outputId": "1f6ea1c4-0914-4891-f600-96a775d9b1e6"
   },
   "outputs": [],
   "source": [
    "matching_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cv15109bKDQG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTNtVJA6Rg18"
   },
   "source": [
    "# val dataset prep..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 494,
     "status": "error",
     "timestamp": 1588442105310,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "ynbxSBkeKE8K",
    "outputId": "d0e404ba-4753-47ed-aa11-05c7baba3dd6"
   },
   "outputs": [],
   "source": [
    "boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8XRWV8yKEys"
   },
   "outputs": [],
   "source": [
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "boxes = np.load('may_boxes.npy')\n",
    "np.load = np_load_old\n",
    "boxes = list(boxes)\n",
    "\n",
    "# data for first 1000 images for val\n",
    "boxes = boxes[5000:6000]\n",
    "\n",
    "# convert bbox array to have same shape for each image data\n",
    "# accomodate for the maximum annotations for the dataset \n",
    "# put zeros for extra annotations\n",
    "\n",
    "ann_shape = []\n",
    "for i in boxes:\n",
    "    ann_shape.append(i.shape[0]) # 0th position holds the number of annotations per image\n",
    "\n",
    "max_ann_shape = max(ann_shape)\n",
    "equal_shape_boxes = np.zeros((1000,max_ann_shape,num_anchors))\n",
    "\n",
    "for index, box in enumerate(boxes):\n",
    "    equal_shape_boxes[index,:box.shape[0],:num_anchors] = box # saving the annotations for each image and allowing remaining annotations to be zero \n",
    "\n",
    "\n",
    "\n",
    "# prepare and load the image data\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "fn = np.load('may_fns.npy')\n",
    "np.load = np_load_old\n",
    "\n",
    "\n",
    "fns = fn.copy()\n",
    "fns = ['images/'+ i for i in fns]\n",
    "fns = fns[5000:6000]\n",
    "file_names = np.array(fns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7Wjgjt1iZQE"
   },
   "outputs": [],
   "source": [
    "# create ground truth labels for training and processing results \n",
    "\n",
    "h_norm = 1/720\n",
    "w_norm = 1/1280\n",
    "anchors = np.array(anchors)\n",
    "anchors = anchors.reshape(num_anchors, 2)\n",
    "\n",
    "#\n",
    "all_matching_boxes = []\n",
    "all_detector_masks = []\n",
    "all_bboxes = []\n",
    "\n",
    "for i in range(equal_shape_boxes.shape[0]):\n",
    "\n",
    "    detector_mask = np.zeros((grid_w,grid_h,num_anchors,1))\n",
    "    matching_boxes = np.zeros((grid_w,grid_h,num_anchors,5))\n",
    "    grid_boxes = np.zeros(equal_shape_boxes.shape[1:])\n",
    "  \n",
    "    for j,b in  enumerate(equal_shape_boxes[i,:,:]):\n",
    "    \n",
    "    #convert pixel values annotations to yolo output resolution \n",
    "        w = (b[2] - b[0]) * w_norm * grid_w\n",
    "        h = (b[3] - b[1]) * h_norm * grid_h   \n",
    "        x = ((b[0] + b[2]) / 2) * w_norm * grid_w\n",
    "        y = ((b[1] + b[3]) / 2) * h_norm * grid_h\n",
    "        grid_boxes[j,...] = np.array([x,y,w,h,b[4]])\n",
    "\n",
    "        if x > 0: # if box annotation exists\n",
    "\n",
    "          # picking the best anchor for given bbox\n",
    "            iou_best = 0\n",
    "            anchor_best = 0\n",
    "            for k in range(num_anchors):\n",
    "                intersection = np.minimum(w,anchors[k,0]) * np.minimum(h,anchors[k,1])\n",
    "                union = (anchors[k,0] * anchors[k,1]) + (w * h) - intersection\n",
    "                iou = intersection / union\n",
    "\n",
    "                if iou > iou_best:\n",
    "                    iou_best = iou\n",
    "                    anchor_best = k\n",
    "            if iou_best > 0:\n",
    "\n",
    "                o_x = np.floor(x).astype('int')\n",
    "                o_y = np.floor(y).astype('int')\n",
    "                detector_mask[o_y, o_x, anchor_best] = 1\n",
    "                matching_boxes[o_y, o_x, anchor_best] = grid_boxes[j]\n",
    "\n",
    "    all_matching_boxes.append(matching_boxes)\n",
    "    all_detector_masks.append(detector_mask)\n",
    "    all_bboxes.append(grid_boxes)\n",
    "\n",
    "detector_masks = tf.convert_to_tensor(np.array(all_detector_masks), dtype='float32')\n",
    "matching_bboxes = tf.convert_to_tensor(np.array(all_matching_boxes),dtype='float32')\n",
    "bboxes = tf.convert_to_tensor(np.array(all_bboxes), dtype='float32')\n",
    "\n",
    "matching_classes = K.cast(matching_bboxes[..., 4], 'int32') \n",
    "class_one_hot = K.one_hot(matching_classes, num_class + 1)[:,:,:,:,1:]\n",
    "class_one_hot = tf.cast(class_one_hot, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYZ07Ng_KEli"
   },
   "outputs": [],
   "source": [
    "def parse_image_1(obj, detector_masks, matching_bboxes, class_one_hot, bboxes):\n",
    "    img_obj = tf.io.read_file(obj)\n",
    "    imgs = tf.image.decode_png(img_obj, channels=3)\n",
    "    imgs = tf.image.convert_image_dtype(imgs, tf.float32)\n",
    "    imgs = tf.image.resize(imgs,[416,416])\n",
    "    return imgs, detector_masks, matching_bboxes, class_one_hot, bboxes\n",
    "\n",
    "dataset_2 = tf.data.Dataset.from_tensor_slices((file_names,detector_masks, matching_bboxes, class_one_hot, bboxes))\n",
    "dataset_2 = dataset_2.map(parse_image_1, num_parallel_calls=6)\n",
    "dataset_2 = dataset_2.shuffle(len(file_names), reshuffle_each_iteration=True)\n",
    "dataset_2 = dataset_2.repeat()\n",
    "dataset_2 = dataset_2.batch(25)\n",
    "dataset_2 = dataset_2.prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22pce2OZKEa-"
   },
   "outputs": [],
   "source": [
    "def train_batch(dataset):\n",
    "    for batch in dataset:\n",
    "        (imgs, detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid) = batch[0],batch[1],batch[2],batch[3],batch[4]\n",
    "        batch = (imgs, detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid)\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qS_Qy-YmKEVh"
   },
   "outputs": [],
   "source": [
    "train_gen = train_batch(dataset_1)\n",
    "val_gen = train_batch(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYR21O9-il44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYaT52coiqGE"
   },
   "source": [
    "# detection specific codes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S__GLpDKipgZ"
   },
   "outputs": [],
   "source": [
    "def iou(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    '''\n",
    "    Calculate IOU between box1 and box2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - x, y : box center coords\n",
    "    - w : box width\n",
    "    - h : box height\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - IOU\n",
    "    '''   \n",
    "    xmin1 = x1 - 0.5*w1\n",
    "    xmax1 = x1 + 0.5*w1\n",
    "    ymin1 = y1 - 0.5*h1\n",
    "    ymax1 = y1 + 0.5*h1\n",
    "    xmin2 = x2 - 0.5*w2\n",
    "    xmax2 = x2 + 0.5*w2\n",
    "    ymin2 = y2 - 0.5*h2\n",
    "    ymax2 = y2 + 0.5*h2\n",
    "    interx = np.minimum(xmax1, xmax2) - np.minimum(xmin1, xmin2)\n",
    "    intery = np.minimum(ymax1, ymax2) - np.minimum(ymin1, ymin2)\n",
    "    inter = interx * intery\n",
    "    union = w1*h1 + w2*h2 - inter\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "\n",
    "# loss\n",
    "\n",
    "def yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid, y_pred):\n",
    "    '''\n",
    "    Calculate YOLO V2 loss from prediction (y_pred) and ground truth tensors (detector_mask,\n",
    "    matching_true_boxes, class_one_hot, true_boxes_grid,)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - detector_mask : tensor, shape (batch, size, GRID_W, GRID_H, anchors_count, 1)\n",
    "        1 if bounding box detected by grid cell, else 0\n",
    "    - matching_true_boxes : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 5)\n",
    "        Contains adjusted coords of bounding box in YOLO format\n",
    "    - class_one_hot : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, class_count)\n",
    "        One hot representation of bounding box label\n",
    "    - true_boxes_grid : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "        true_boxes_grid format : x, y, w, h, c (coords unit : grid cell)\n",
    "    - y_pred : prediction from model. tensor (shape : batch_size, GRID_W, GRID_H, anchors count, (5 + labels count)\n",
    "    - info : boolean. True to get some infox about loss value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss : scalar\n",
    "    - sub_loss : sub loss list : coords loss, class loss and conf loss : scalar\n",
    "    '''\n",
    "    \n",
    "    # grid coords tensor\n",
    "    coord_x = tf.cast(tf.reshape(tf.tile(tf.range(16), [16]), (1, 16, 16, 1, 1)), tf.float32)\n",
    "    coord_y = tf.transpose(coord_x, (0,2,1,3,4))\n",
    "    coords = tf.tile(tf.concat([coord_x,coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "    \n",
    "    # coordinate loss\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2]) # adjust coords between 0 and 1\n",
    "    pred_xy = (pred_xy + coords) # add cell coord for comparaison with ground truth. New coords in grid cell unit\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors # adjust width and height for comparaison with ground truth. New coords in grid cell unit\n",
    "    #pred_wh = (pred_wh * anchors) # unit : grid cell\n",
    "    nb_detector_mask = K.sum(tf.cast(detector_mask > 0.0, tf.float32))\n",
    "    xy_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(matching_true_boxes[...,:2] - pred_xy)) / (nb_detector_mask + 1e-6) # Non /2\n",
    "    wh_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(K.sqrt(matching_true_boxes[...,2:4]) - \n",
    "                                                            K.sqrt(pred_wh))) / (nb_detector_mask + 1e-6)\n",
    "    coord_loss = xy_loss + wh_loss\n",
    "    \n",
    "    # class loss    \n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    true_box_class = tf.argmax(class_one_hot, -1)\n",
    "    #class_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    class_loss = K.sparse_categorical_crossentropy(target=true_box_class, output=pred_box_class, from_logits=True)\n",
    "    class_loss = K.expand_dims(class_loss, -1) * detector_mask\n",
    "    class_loss = LAMBDA_CLASS * K.sum(class_loss) / (nb_detector_mask + 1e-6)\n",
    "    \n",
    "    # confidence loss\n",
    "    pred_conf = K.sigmoid(y_pred[...,4:5])\n",
    "    # for each detector : iou between prediction and ground truth\n",
    "    x1 = matching_true_boxes[...,0]\n",
    "    y1 = matching_true_boxes[...,1]\n",
    "    w1 = matching_true_boxes[...,2]\n",
    "    h1 = matching_true_boxes[...,3]\n",
    "    x2 = pred_xy[...,0]\n",
    "    y2 = pred_xy[...,1]\n",
    "    w2 = pred_wh[...,0]\n",
    "    h2 = pred_wh[...,1]\n",
    "    ious = iou(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    ious = K.expand_dims(ious, -1)\n",
    "     \n",
    "    # for each detector : best ious between prediction and true_boxes (every bounding box of image)\n",
    "    pred_xy = K.expand_dims(pred_xy, 4) # shape : m, GRID_W, GRID_H, BOX, 1, 2 \n",
    "    pred_wh = K.expand_dims(pred_wh, 4)\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins = pred_xy - pred_wh_half\n",
    "    pred_maxes = pred_xy + pred_wh_half\n",
    "    true_boxe_shape = K.int_shape(true_boxes_grid)\n",
    "    true_boxes_grid = K.reshape(true_boxes_grid, [true_boxe_shape[0], 1, 1, 1, true_boxe_shape[1], true_boxe_shape[2]])\n",
    "    true_xy = true_boxes_grid[...,0:2]\n",
    "    true_wh = true_boxes_grid[...,2:4]\n",
    "    true_wh_half = true_wh * 0.5\n",
    "    true_mins = true_xy - true_wh_half\n",
    "    true_maxes = true_xy + true_wh_half\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2 \n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.) # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, 1, 1\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # Best IOU scores.\n",
    "    best_ious = K.expand_dims(best_ious) # shape : m, GRID_W, GRID_H, BOX, 1\n",
    "    \n",
    "    # no object confidence loss\n",
    "    no_object_detection = K.cast(best_ious < 0.6, K.dtype(best_ious)) \n",
    "    noobj_mask = no_object_detection * (1 - detector_mask)\n",
    "    nb_noobj_mask  = K.sum(tf.cast(noobj_mask  > 0.0, tf.float32))\n",
    "    \n",
    "    noobject_loss =  LAMBDA_NOOBJECT * K.sum(noobj_mask * K.square(-pred_conf)) / (nb_noobj_mask + 1e-6)\n",
    "    # object confidence loss\n",
    "    object_loss = LAMBDA_OBJECT * K.sum(detector_mask * K.square(ious - pred_conf)) / (nb_detector_mask + 1e-6)\n",
    "    # total confidence loss\n",
    "    conf_loss = noobject_loss + object_loss\n",
    "    \n",
    "    # total loss\n",
    "    loss = conf_loss + class_loss + coord_loss\n",
    "    sub_loss = [conf_loss, class_loss, coord_loss]  \n",
    "    \n",
    "#     # 'triple' mask\n",
    "#     true_box_conf_IOU = ious * detector_mask\n",
    "#     conf_mask = noobj_mask * LAMBDA_NOOBJECT\n",
    "#     conf_mask = conf_mask + detector_mask * LAMBDA_OBJECT\n",
    "#     nb_conf_box  = K.sum(tf.to_float(conf_mask  > 0.0))\n",
    "#     conf_loss = K.sum(K.square(true_box_conf_IOU - pred_conf) * conf_mask)  / (nb_conf_box  + 1e-6) \n",
    "    \n",
    "#     # total loss\n",
    "#     loss = conf_loss /2. + class_loss + coord_loss /2.\n",
    "#     sub_loss = [conf_loss /2., class_loss, coord_loss /2.]\n",
    "\n",
    "              \n",
    "    return loss, sub_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ey9ShkIQizYA"
   },
   "outputs": [],
   "source": [
    "# gradients\n",
    "def grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(img, training)\n",
    "        loss, sub_loss = yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes, y_pred)\n",
    "    return loss, sub_loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# save weights\n",
    "\n",
    "def save_best_weights(model, name, epoch):\n",
    "\n",
    "    name = name + '_' + str(epoch+1) + '.h5'\n",
    "    path_name = os.path.join('Weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "\n",
    "# log (tensorboard)\n",
    "#def log_loss(loss, val_loss, step):\n",
    "    #tf.summary.scalar('loss', loss, step)\n",
    "    #tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptoRaB1OjDAw"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, steps_per_epoch_val, train_name = 'train-1'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join('logs/', train_name), flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train): \n",
    "            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(train_dataset)\n",
    "            loss, _, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "\n",
    "            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(val_dataset)\n",
    "            loss, sub_loss, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=False)\n",
    "            epoch_val_loss.append(loss)\n",
    "            epoch_val_sub_loss.append(sub_loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        sub_loss_avg = np.mean(np.array(epoch_val_sub_loss), axis=0)\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)        \n",
    "        # log\n",
    "        #log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg       \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f}'.format(loss_avg,val_loss_avg))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeuFYd3MjDvY"
   },
   "outputs": [],
   "source": [
    "LAMBDA_NOOBJECT  = 1\n",
    "LAMBDA_OBJECT    = 5\n",
    "LAMBDA_CLASS     = 1\n",
    "LAMBDA_COORD     = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 150795,
     "status": "error",
     "timestamp": 1588458606032,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "fZStS4EAjTfq",
    "outputId": "8876a88c-b50c-40e0-ec3f-4f2f93b71fe5"
   },
   "outputs": [],
   "source": [
    "results = train(50, new_model, train_gen, val_gen, 200, 40 , 'yolo') # 1e-3\n",
    "plt.plot(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkVKwZ6PjTJn"
   },
   "outputs": [],
   "source": [
    "training_graph = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x__m6uDjmVE7"
   },
   "outputs": [],
   "source": [
    "training_graph.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1485,
     "status": "ok",
     "timestamp": 1588461052665,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "XTZwprnYmU6-",
    "outputId": "effb7a84-eb58-434f-be94-f54a37bad898"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "cv2_imshow(cv2.imread(file_names[109]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1588460896048,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "7Fye_r2upa5H",
    "outputId": "84422abb-0028-45aa-f278-56db13f7c784"
   },
   "outputs": [],
   "source": [
    "file_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1588468298516,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "7RRYqbC0pbvM",
    "outputId": "1183f0b4-2d90-45f9-841e-0a1d733ea5bd"
   },
   "outputs": [],
   "source": [
    "'/content/data/images/'+fn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 381,
     "status": "error",
     "timestamp": 1588468346021,
     "user": {
      "displayName": "Rajarathnam Balakrishnan",
      "photoUrl": "",
      "userId": "09787588341109245264"
     },
     "user_tz": 420
    },
    "id": "tncgqa62Fqd1",
    "outputId": "0bb0fe99-3026-4648-8949-a85b36d4c66e"
   },
   "outputs": [],
   "source": [
    "cv2_imshow(cv2.imread('/content/data/images/'+fn[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE3MxC4jF0OR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMl+szIgNB9cc8QLggGEwvG",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DenseNet+YOLO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}